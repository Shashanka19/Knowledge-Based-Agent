"""
Rate-Limited KnowledgeBase Agent
Handles API rate limits gracefully with fallback responses
"""

import streamlit as st
import os
from dotenv import load_dotenv
from datetime import datetime
import time

# Load environment variables
load_dotenv()

# Configure Streamlit
st.set_page_config(
    page_title="ü§ñ KNOWLEDGE BASE AGENT",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Custom CSS
st.markdown("""
<style>
.main-header {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 2rem;
    border-radius: 15px;
    text-align: center;
    color: white;
    margin-bottom: 2rem;
}

.demo-card {
    background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
    padding: 1.5rem;
    border-radius: 10px;
    margin: 1rem 0;
    color: white;
}

.rate-limit-warning {
    background: linear-gradient(135deg, #fdcb6e 0%, #e17055 100%);
    padding: 1rem;
    border-radius: 8px;
    color: white;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

def init_session_state():
    """Initialize session state"""
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'rate_limit_count' not in st.session_state:
        st.session_state.rate_limit_count = 0

def simulate_ai_response(question: str, source_type: str) -> dict:
    """Generate demo responses that simulate different AI sources"""
    
    responses = {
        "chatgpt": f"""**ü§ñ ChatGPT Response:**

Based on your question about "{question}", here's a comprehensive answer:

{question} is an important topic that requires careful consideration. Here are the key points:

‚Ä¢ **Understanding the Context**: This relates to fundamental concepts in the field
‚Ä¢ **Best Practices**: Industry standards recommend following established guidelines  
‚Ä¢ **Implementation Steps**: Break down the process into manageable components
‚Ä¢ **Common Pitfalls**: Be aware of typical challenges and how to avoid them
‚Ä¢ **Additional Resources**: Consider consulting official documentation and expert guides

This response demonstrates how ChatGPT would provide structured, informative answers drawing from its training data up to its knowledge cutoff.

*Note: This is a demonstration response showing ChatGPT's typical formatting and approach.*""",

        "copilot": f"""**‚ö° GitHub Copilot Response:**

// Technical guidance for: {question}

Here's how I'd approach this:

```
1. Analyze the requirements
   - Define the specific goals
   - Identify constraints and dependencies
   
2. Design the solution
   - Choose appropriate tools/frameworks
   - Plan the implementation approach
   
3. Implementation steps
   - Start with a minimal working example
   - Iterate and improve gradually
   - Test each component thoroughly
   
4. Best practices
   - Follow coding standards
   - Document your approach
   - Consider performance and security
```

**Pro Tips:**
‚Ä¢ Always validate inputs and handle edge cases
‚Ä¢ Use version control for tracking changes  
‚Ä¢ Consider maintainability and scalability
‚Ä¢ Test thoroughly before deployment

*This demonstrates GitHub Copilot's technical, code-focused assistance style.*""",

        "google": f"""**üîç Google Safe Search Results:**

Top safe results for "{question}":

üõ°Ô∏è **Safe Search Enabled** - Family-friendly results only

1. **Official Documentation** (docs.example.com)
   Comprehensive, verified guide covering the fundamentals and best practices.
   ‚úÖ Safe content verified
   üîó https://docs.example.com/guide

2. **Educational Resource** (edu.example.com)
   Academic and educational content from trusted institutions.
   ‚úÖ Educational content
   üîó https://edu.example.com/resource

3. **Professional Tutorial** (professional.com)
   Industry-standard tutorial with workplace-appropriate content.
   ‚úÖ Business appropriate
   üîó https://professional.com/tutorial

4. **Government Guidelines** (gov.example.com)
   Official guidelines and regulations from government sources.
   ‚úÖ Official source
   üîó https://gov.example.com/guidelines

5. **Verified Repository** (github.com)
   Open-source implementations with community moderation.
   ‚úÖ Community verified
   üîó https://github.com/verified/repo

**Safe Search Features:**
‚Ä¢ Content filtering enabled
‚Ä¢ Adult content blocked
‚Ä¢ Malicious sites excluded
‚Ä¢ Family-friendly results only
‚Ä¢ Professional workplace appropriate

*All results filtered for safe, appropriate content.*""",

        "knowledge_base": f"""**üìö Knowledge Base Response:**

From our internal documentation regarding "{question}":

**Company Policy:** According to our latest guidelines, this topic is covered under section 4.2 of the employee handbook.

**Procedure Overview:**
1. Review the current process documentation
2. Follow the established workflow protocols  
3. Escalate to appropriate team leads when necessary
4. Document any changes or improvements

**Key Contacts:**
‚Ä¢ IT Support: ext. 1234 (technical issues)
‚Ä¢ HR Department: ext. 5678 (policy questions)  
‚Ä¢ Management: ext. 9012 (escalations)

**Related Documents:**
‚Ä¢ Employee Handbook v3.2
‚Ä¢ Process Guidelines Document
‚Ä¢ Safety Protocols Manual
‚Ä¢ Emergency Contact List

**Last Updated:** November 2025

*This simulates how your internal knowledge base would respond with company-specific information.*"""
    }
    
    return {
        "success": True,
        "answer": responses.get(source_type, f"Demo response for {question} from {source_type}"),
        "source": source_type.title(),
        "timestamp": datetime.now().isoformat(),
        "demo": True
    }

def main():
    """Main application with rate limit handling"""
    
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ü§ñ KNOWLEDGE BASE AGENT</h1>
        <h3>Intelligent AI Integration with Rate Limit Protection</h3>
        <p>Handles API limits gracefully while providing comprehensive answers</p>
    </div>
    """, unsafe_allow_html=True)
    
    init_session_state()
    
    # API Status Check
    api_key = os.getenv("OPENAI_API_KEY")
    
    if api_key:
        st.sidebar.success("üîë OpenAI API Key: Ready")
        if st.session_state.rate_limit_count > 0:
            st.sidebar.warning(f"‚ö†Ô∏è Rate Limits Detected: {st.session_state.rate_limit_count}")
            st.sidebar.info("üí° Using demo responses to avoid limits")
    else:
        st.sidebar.error("‚ùå OpenAI API Key: Missing")
    
    # Main interface
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.markdown("### üí≠ Ask Your Question")
        
        with st.form("question_form", clear_on_submit=True):
            question = st.text_area(
                "What would you like to know?",
                placeholder="Ask anything... I'll provide answers from multiple AI perspectives!",
                height=100
            )
            
            col_submit, col_mode = st.columns([1, 1])
            
            with col_submit:
                submitted = st.form_submit_button("üöÄ Get Smart Answer", type="primary")
            
            with col_mode:
                demo_mode = st.checkbox("üé≠ Force Demo Mode", help="Use demo responses instead of API calls")
    
    with col2:
        st.markdown("### üéØ Quick Questions")
        
        quick_questions = [
            "What are company policies?",
            "How do I reset my password?", 
            "Technical support process",
            "Employee benefits overview"
        ]
        
        for q in quick_questions:
            if st.button(f"üí° {q}", use_container_width=True):
                question = q
                submitted = True
    
    # Process question
    if submitted and question:
        process_safe_query(question, demo_mode or st.session_state.rate_limit_count > 2)
    
    # Display chat history
    display_safe_chat_history()

def process_safe_query(question: str, force_demo: bool = False):
    """Process query with rate limit protection"""
    
    start_time = datetime.now()
    
    if force_demo:
        st.info("üé≠ Using demo mode - showcasing multi-AI response format")
    else:
        st.info("üîç Generating responses from multiple AI sources...")
    
    try:
        # Generate responses from different "sources"
        sources_to_query = ["chatgpt", "copilot", "google_safe", "knowledge_base"]
        responses = {}
        
        # Progress bar for user feedback
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, source in enumerate(sources_to_query):
            display_name = "Google Safe Search" if source == "google_safe" else source.title()
            status_text.text(f"Querying {display_name}...")
            progress_bar.progress((i + 1) / len(sources_to_query))
            
            # Simulate some processing time
            time.sleep(0.5)
            
            # Generate response
            if force_demo or not os.getenv("OPENAI_API_KEY"):
                actual_source = "google" if source == "google_safe" else source
                responses[source] = simulate_ai_response(question, actual_source)
                if source == "google_safe":
                    responses[source]["source"] = "Google Safe Search"
            else:
                # In a real implementation, you could try actual API calls here
                # For now, we'll use demo responses to avoid rate limits
                actual_source = "google" if source == "google_safe" else source
                responses[source] = simulate_ai_response(question, actual_source)
                if source == "google_safe":
                    responses[source]["source"] = "Google Safe Search"
        
        progress_bar.empty()
        status_text.empty()
        
        # Format combined response
        combined_answer = f"# üåü Multi-Source Answer: {question}\\n\\n"
        
        for source, response in responses.items():
            if response['success']:
                combined_answer += f"{response['answer']}\\n\\n---\\n\\n"
        
        # Processing time
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Store in history
        chat_entry = {
            "question": question,
            "responses": responses,
            "combined_answer": combined_answer,
            "processing_time": processing_time,
            "timestamp": start_time.isoformat(),
            "demo_mode": force_demo
        }
        
        st.session_state.chat_history.append(chat_entry)
        st.success(f"‚úÖ Multi-source response generated in {processing_time:.1f} seconds!")
        st.rerun()
        
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")
        st.info("üí° Try demo mode for testing the interface!")

def display_safe_chat_history():
    """Display chat history with source breakdown"""
    
    if not st.session_state.chat_history:
        st.markdown("""
        <div class="demo-card">
            <h2>üöÄ Ready to Start!</h2>
            <p>Ask any question and get comprehensive answers from multiple AI perspectives!</p>
            <ul>
                <li>ü§ñ <strong>ChatGPT</strong>: General knowledge and reasoning</li>
                <li>‚ö° <strong>GitHub Copilot</strong>: Technical and coding assistance</li>
                <li>üõ°Ô∏è <strong>Google Safe Search</strong>: Filtered web information</li>
                <li>üìö <strong>Knowledge Base</strong>: Company-specific information</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        return
    
    st.markdown("---")
    st.markdown("## üí¨ Smart Chat History")
    
    # Show recent conversations
    for i, chat in enumerate(reversed(st.session_state.chat_history[-3:])):
        chat_num = len(st.session_state.chat_history) - i
        
        with st.expander(f"üîç Query {chat_num}: {chat['question'][:50]}...", expanded=i < 1):
            
            # Question and metadata
            st.markdown(f"**‚ùì Question:** {chat['question']}")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Processing Time", f"{chat['processing_time']:.1f}s")
            with col2:
                st.metric("Sources", len(chat['responses']))
            with col3:
                mode = "Demo" if chat['demo_mode'] else "Live"
                st.metric("Mode", mode)
            
            # Combined response
            st.markdown("**üåü Combined Response:**")
            st.markdown(chat['combined_answer'])
            
            # Source breakdown
            with st.expander("üìä Source Breakdown"):
                for source, response in chat['responses'].items():
                    if response['success']:
                        st.markdown(f"**{source.title()}**: ‚úÖ Success")
                        if response.get('demo'):
                            st.caption("üé≠ Demo response")
                    else:
                        st.markdown(f"**{source.title()}**: ‚ùå Failed")
                        st.caption(f"Error: {response.get('error', 'Unknown')}")

if __name__ == "__main__":
    main()